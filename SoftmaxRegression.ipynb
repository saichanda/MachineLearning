{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from sklearn.metrics import accuracy_score\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_cost(theta, n_classes, input_size, lambda_, data, labels):\n",
    "    \"\"\"\n",
    "    Compute the cost and derivative.\n",
    "    n_classes: the number of classes\n",
    "    input_size: the size N of the input vector\n",
    "    lambda_: weight decay parameter\n",
    "    data: the N x M input matrix, where each column data[:, i] corresponds to\n",
    "          a single test set\n",
    "    labels: an M x 1 matrix containing the labels corresponding for the input data\n",
    "    \"\"\"\n",
    "    # k stands for the number of classes\n",
    "    # n is the number of features and m is the number of samples\n",
    "    k = n_classes\n",
    "    n, m = data.shape\n",
    "    # Reshape theta\n",
    "    theta = theta.reshape((k, n))\n",
    "    # Probability with shape (k, m)\n",
    "    theta_data = theta.dot(data)\n",
    "    alpha = np.max(theta_data, axis=0)\n",
    "    theta_data -= alpha # Avoid numerical problem due to large values of exp(theta_data)\n",
    "    proba = np.exp(theta_data) / np.sum(np.exp(theta_data), axis=0)\n",
    "    # Matrix of indicator fuction with shape (k, m)\n",
    "    labels = np.reshape(labels,(m,))\n",
    "    indicator = scipy.sparse.csr_matrix((np.ones(m), (labels, np.arange(m))))\n",
    "    indicator = np.array(indicator.todense())\n",
    "    # Cost function\n",
    "    cost = -1.0/m * np.sum(indicator * np.log(proba)) + 0.5*lambda_*np.sum(theta*theta)\n",
    "    # Gradient matrix with shape (k, n)\n",
    "    grad = -1.0/m * (indicator - proba).dot(data.T) + lambda_*theta\n",
    "    # Unroll the gradient matrices into a vector\n",
    "    grad = grad.ravel()\n",
    "    return cost, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Gradient(J,theta, lr, maxiter):\n",
    "    for i in range(maxiter):\n",
    "        _,gradient = J(theta)\n",
    "        theta = theta - (lr * gradient)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_train(input_size, n_classes, lambda_,lr,  input_data, labels, options={'maxiter': 100, 'disp': True}):\n",
    "    \"\"\"\n",
    "    Train a softmax model with the given parameters on the given data.\n",
    "    Returns optimal theta, a vector containing the trained parameters\n",
    "    for the model.\n",
    "    input_size: the size of an input vector x^(i)\n",
    "    n_classes: the number of classes\n",
    "    lambda_: weight decay parameter\n",
    "    input_data: an N by M matrix containing the input data, such that\n",
    "                input_data[:, c] is the c-th input\n",
    "    labels: M by 1 matrix containing the class labels for the\n",
    "            corresponding inputs. labels[c] is the class label for\n",
    "            the c-th input\n",
    "    options (optional): options\n",
    "      options['maxiter']: number of iterations to train for\n",
    "    \"\"\"\n",
    "    # initialize parameters\n",
    "    theta = 0.005 * np.random.randn(n_classes * input_size)\n",
    "    # Cost function (returns as a gradient)\n",
    "    J = lambda theta : softmax_cost(theta, n_classes, input_size, lambda_, input_data, labels)\n",
    "    # Find out the optimal theta\n",
    "    #results = scipy.optimize.minimize(J, theta, method='L-BFGS-B', jac=True, options=options)\n",
    "    #opt_theta = results['x']\n",
    "    opt_theta = Gradient(J,theta, lr, maxiter = options[\"maxiter\"])\n",
    "    model = {'opt_theta': opt_theta, 'n_classes': n_classes, 'input_size': input_size}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_predict(model, data):\n",
    "    \"\"\"\n",
    "    model: model trained using softmax_train\n",
    "    data: the N x M input matrix, where each column data[:, i] corresponds to\n",
    "          a single test set\n",
    "    pred: the prediction array.\n",
    "    \"\"\"\n",
    "    theta = model['opt_theta'] # Optimal theta\n",
    "    k = model['n_classes']  # Number of classes\n",
    "    n = model['input_size'] # Input size (number of features)\n",
    "    # Reshape theta\n",
    "    theta = theta.reshape((k, n))\n",
    "    # Probability with shape (k, m)\n",
    "    theta_data = theta.dot(data)\n",
    "    alpha = np.max(theta_data, axis=0)\n",
    "    theta_data -= alpha # Avoid numerical problem due to large values of exp(theta_data)\n",
    "    proba = np.exp(theta_data) / np.sum(np.exp(theta_data), axis=0)\n",
    "    # Prediction values\n",
    "    pred = np.argmax(proba, axis=0)\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000)\n",
      "(784, 10000)\n"
     ]
    }
   ],
   "source": [
    "X = loadmat('./Data/mnistTrainImages.mat')\n",
    "X = X['trainData'].T\n",
    "y = loadmat('./Data/mnistTrainLabels.mat')\n",
    "y = y['trainLabels']\n",
    "X_test = loadmat('./Data/mnistTestImages.mat')\n",
    "X_test = X_test['testData'].T\n",
    "y_test = loadmat('./Data/mnistTestLabels.mat')\n",
    "y_test = y_test['testLabels']\n",
    "print (X.shape)\n",
    "print (X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = 28 * 28 # Size of input vector (MNIST images are 28x28)\n",
    "n_classes = 10       # Number of classes (MNIST images fall into 10 classes)\n",
    "lambda_ = 1e-4 # Weight decay parameter ... Hyper parameter and lambda is also  the hyper parameter.\n",
    "lr = .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "options = {'maxiter': 100, 'disp': True}\n",
    "model = softmax_train(input_size, n_classes, lambda_,lr, X, y, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.773133333333\n"
     ]
    }
   ],
   "source": [
    "pred = softmax_predict(model, X)\n",
    "pred = pred.tolist()\n",
    "print (accuracy_score(y,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Conclusions and remarks :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "|# Iterations|lr(learning_rate)|Accuracy|\n",
    "|:---:|:---:|:---:|\n",
    "|1000|.01|86.02|\n",
    "|1000|.1|90.29|\n",
    "|1000|1|92.36|\n",
    "|500|.01|83.97|\n",
    "|500|.1|89.33|\n",
    "|500|1|91.88|\n",
    "|100|.01|77.31|\n",
    "|100|.1|86.02|\n",
    "|100|1|90.3|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this application, we get good accuracy when run 1000 iterations and with a learning rate of 1.0 as we can see that we get an accuracy of 92.36"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
