]0;IPython: MlProject/fashionmnist_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 13, 13, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 5, 5, 64)          0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 3, 3, 128)         73856     
_________________________________________________________________
dropout_3 (Dropout)          (None, 3, 3, 128)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 1152)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               147584    
_________________________________________________________________
dropout_4 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 10)                1290      
=================================================================
Total params: 241,546
Trainable params: 241,546
Non-trainable params: 0
_________________________________________________________________
Train on 48000 samples, validate on 12000 samples
Epoch 1/50
48000/48000 [==============================] - 29s 613us/step - loss: 0.9357 - acc: 0.6706 - val_loss: 0.5176 - val_acc: 0.8052
Epoch 2/50
48000/48000 [==============================] - 28s 587us/step - loss: 0.5742 - acc: 0.7830 - val_loss: 0.4611 - val_acc: 0.8147
Epoch 3/50
48000/48000 [==============================] - 27s 565us/step - loss: 0.5239 - acc: 0.8049 - val_loss: 0.4327 - val_acc: 0.8369
Epoch 4/50
48000/48000 [==============================] - 27s 565us/step - loss: 0.4863 - acc: 0.8171 - val_loss: 0.3906 - val_acc: 0.8527
Epoch 5/50
48000/48000 [==============================] - 27s 563us/step - loss: 0.4654 - acc: 0.8273 - val_loss: 0.3741 - val_acc: 0.8555
...
Epoch 46/50
48000/48000 [==============================] - 30s 623us/step - loss: 0.3069 - acc: 0.8867 - val_loss: 0.2781 - val_acc: 0.8948
Epoch 47/50
48000/48000 [==============================] - 28s 587us/step - loss: 0.3000 - acc: 0.8878 - val_loss: 0.2782 - val_acc: 0.8939
Epoch 48/50
48000/48000 [==============================] - 28s 590us/step - loss: 0.3046 - acc: 0.8883 - val_loss: 0.2686 - val_acc: 0.8968
Epoch 49/50
48000/48000 [==============================] - 28s 588us/step - loss: 0.2958 - acc: 0.8895 - val_loss: 0.2612 - val_acc: 0.9035
Epoch 50/50
48000/48000 [==============================] - 28s 585us/step - loss: 0.3001 - acc: 0.8892 - val_loss: 0.2571 - val_acc: 0.9028
Test loss: 0.23933553565740584
Test accuracy: 0.9121
             precision    recall  f1-score   support

    Class 0       0.83      0.90      0.86      1000
    Class 1       0.99      0.98      0.99      1000
    Class 2       0.93      0.81      0.86      1000
    Class 3       0.90      0.95      0.92      1000
    Class 4       0.84      0.89      0.87      1000
    Class 5       0.99      0.97      0.98      1000
    Class 6       0.75      0.70      0.72      1000
    Class 7       0.95      0.97      0.96      1000
    Class 8       0.97      0.99      0.98      1000
    Class 9       0.96      0.97      0.97      1000

avg / total       0.91      0.91      0.91     10000

